"""
åŸºç¡€ç‰ˆæœ¬ + è¯­ä¹‰ç†è§£ä¸æ‘˜è¦|SemanticAnalysis + é€šè¿‡URLä¸‹è½½è§†é¢‘ï¼ˆæ¨¡å—åŒ–ï¼‰
æ­¤ç‰ˆæœ¬æœªç»“åˆï¼šè¯­ä¹‰ç†è§£ä¸æ‘˜è¦|SemanticAnalysisLLM æ™ºèƒ½è£å‰ªï½œLLM Clipping
"""


import os
import re
import json
import logging
import argparse
import gradio as gr
from funasr import AutoModel                 # FunASR ç»Ÿä¸€å…¥å£ï¼šASR/VAD/æ ‡ç‚¹/è¯´è¯äºº
from videoclipper import VideoClipper        # è¯†åˆ«ã€è£å‰ªã€å­—å¹•ã€å¯¼å‡º
from llm.openai_api import openai_call       # OpenAI/Deepseek/Moonshot
from llm.qwen_api import call_qwen_model     # Qwen ç³»åˆ—
from llm.g4f_openai_api import g4f_openai_call  # å…è´¹ä»£ç†ç±»æ¥å£
from utils.trans_utils import extract_timestamps
from introduction import top_md_1, top_md_3, top_md_4


from tool.video_downloader import (
    download_video_action,
    resolve_video_input,
)


# ============== è¯­ä¹‰ç†è§£ä¸æ‘˜è¦åˆ†æï¼šPrompt ==============
PROMPT_SEMANTIC_FULL = """ä½ æ˜¯ä¸€ä¸ªè§†é¢‘æ‘˜è¦ä¸å‰ªè¾‘ç­–åˆ’åŠ©æ‰‹ã€‚è¾“å…¥ä¸ºè§†é¢‘çš„å®Œæ•´ SRT å­—å¹•ï¼ˆå«æ—¶é—´ä¸æ–‡æœ¬ï¼‰ã€‚è¯·åŸºäºå†…å®¹å®Œæˆï¼š
1) å½’çº³è§†é¢‘ä¸»é¢˜ï¼›
2) æå–è§†é¢‘ä¸­çš„å…³é”®è¯/å…³é”®çŸ­è¯­ï¼ˆå«ä¸“æœ‰åè¯ï¼‰ï¼›
3) è¾“å‡ºç»“æ„åŒ–å¤§çº²ï¼ˆè‹¥èƒ½ä»å†…å®¹æ¨æ–­ç« èŠ‚ï¼Œè¯·ç»™å‡ºæ¯ç« çš„å¤§è‡´èµ·æ­¢æ—¶é—´ï¼‰ï¼›
4) è¯†åˆ«è§†é¢‘ä¸­çš„â€œç²¾å½©ç‰‡æ®µâ€ï¼Œè¦æ±‚ï¼šè¯­ä¹‰å®Œæ•´ã€æ—¶é—´è¿ç»­ã€èƒ½ä»£è¡¨æ ¸å¿ƒè§‚ç‚¹æˆ–æƒ…ç»ªé«˜æ½®ï¼›æ¯æ¡ç»™å‡ºèµ·æ­¢æ—¶é—´(ç§’)ã€0-1 ç½®ä¿¡åº¦åˆ†ã€ç®€è¦ç†ç”±ã€å¼•ç”¨ä¸€å¥åŸæ–‡ã€‚
5) æŠŠä¸“æœ‰åè¯æŒ‰ç±»åˆ«åˆ†åˆ° entities: {person, org, product, tech_term}ã€‚
6) å¦‚æœ‰å¯¹åç»­å‰ªè¾‘æœ‰ç”¨çš„æç¤ºï¼ˆèŠ‚å¥ã€è½¬æŠ˜ã€ç»“è®ºï¼‰ï¼Œå†™åˆ° notesã€‚

ä¸¥æ ¼è¾“å‡ºå•ä¸ª JSONï¼Œä¸è¦è§£é‡Šæˆ–å¤¹æ‚å¤šä½™æ–‡æœ¬ã€‚æ—¶é—´å•ä½ä½¿ç”¨â€œç§’â€ï¼Œå°æ•°ä¿ç•™ä¸¤ä½ã€‚

ä»¥ä¸‹æ˜¯ SRT æ–‡æœ¬ï¼š
"""

def _safe_json_parse(s: str) -> dict:
    """ä» LLM è¾“å‡ºä¸­æå– JSONï¼›å¤±è´¥åˆ™å°è¯•é¦–å°¾èŠ±æ‹¬å·æˆªå–ã€‚"""
    s = (s or "").strip()
    try:
        return json.loads(s)
    except Exception:
        try:
            head, tail = s.find("{"), s.rfind("}")
            if head != -1 and tail != -1 and head < tail:
                return json.loads(s[head:tail+1])
        except Exception:
            pass
        raise ValueError("LLM æœªè¿”å›åˆæ³• JSONã€‚")

def _call_llm_any(apikey: str, model: str, prompt: str, content: str) -> str:
    """ç»Ÿä¸€çš„ LLM è°ƒç”¨å°è£…ï¼šqwen / gpt / moonshot / deepseek / g4f-*"""
    if model.startswith("qwen"):
        return call_qwen_model(apikey, model, user_input=prompt + content, system_input="")
    if model.startswith("gpt") or model.startswith("moonshot") or model.startswith("deepseek"):
        return openai_call(apikey, model, system_content="", user_content=prompt + content)
    if model.startswith("g4f"):
        pure_model = "-".join(model.split("-")[1:])
        return g4f_openai_call(pure_model, "", prompt + content)
    raise ValueError("Unsupported model prefix. Use one of: qwen / gpt / g4f / moonshot / deepseek")

def semantic_analysis_run(srt_text: str, apikey: str, model: str) -> str:
    """
    è¯­ä¹‰ç†è§£ä¸æ‘˜è¦åˆ†æä¸»å…¥å£ï¼š
    - è¾“å…¥ï¼šSRT æ–‡æœ¬ï¼ˆå¸¦æ—¶é—´ä¸å†…å®¹ï¼‰
    - è¾“å‡ºï¼šJSON å­—ç¬¦ä¸²ï¼ˆåŒ…å« topics/keywords/outline/entities/highlights/notesï¼‰
    """
    if not srt_text or not srt_text.strip():
        return json.dumps({"error": "empty srt"}, ensure_ascii=False, indent=2)
    try:
        raw = _call_llm_any(apikey, model, PROMPT_SEMANTIC_FULL, srt_text)
        data = _safe_json_parse(raw)

        # å…œåº•å­—æ®µ
        data.setdefault("topics", [])
        data.setdefault("keywords", [])
        data.setdefault("outline", [])
        data.setdefault("entities", {"person": [], "org": [], "product": [], "tech_term": []})
        data.setdefault("highlights", [])
        data.setdefault("notes", "")

        return json.dumps(data, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.exception(e)
        return json.dumps({
            "topics": [], "keywords": [], "outline": [],
            "entities": {"person": [], "org": [], "product": [], "tech_term": []},
            "highlights": [], "notes": f"fallback: {str(e)}"
        }, ensure_ascii=False, indent=2)

def highlights_to_timestamps(analysis_json_str: str):
    """
    ä»è¯­ä¹‰åˆ†æ JSON ä¸­æŠ½å–é«˜å…‰ç‰‡æ®µä¸º timestamp_list [[start, end], ...]ï¼ˆå•ä½ï¼šç§’ï¼‰
    - è¿‡æ»¤é•¿åº¦ < 2s çš„è¿‡çŸ­ç‰‡æ®µ
    - æœ€å¤šè¿”å› 6 æ®µ
    """
    try:
        data = json.loads(analysis_json_str)
    except Exception:
        return []
    ts = []
    for h in data.get("highlights", []):
        try:
            s = float(h.get("start", 0))
            e = float(h.get("end", 0))
            if e > s and (e - s) >= 2.0:
                ts.append([round(s, 2), round(e, 2)])
        except Exception:
            continue
    return ts[:6]

# ============= æ–°å¢: å…¼å®¹æ—§ç‰ˆ video_clip çš„æ—¶é—´å•ä½ç¼©æ”¾ =============
def _compat_scale_seconds_for_legacy(ts_list):
    """
    ä¼ å…¥: [[start_sec, end_sec], ...]  (å•ä½: ç§’)
    ç›®çš„: å…¼å®¹åº•å±‚ video_clip/clip ä¸­ 'x16 å† /16000' çš„æ—§é€»è¾‘ã€‚
         ä¸ºäº†è®©æœ€ç»ˆæ—¥å¿—ä»ç„¶æ˜¾ç¤ºä¸ºç§’ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå°†ç§’è½¬æˆ"æ¯«ç§’"ä¼ ä¸‹å»ï¼š
         (sec * 1000) * 16 / 16000 = sec
    è¿”å›: [[start_scaled, end_scaled], ...]  (å•ä½: ä¼ ç»™åº•å±‚çš„æ•°å€¼)
    """
    scaled = []
    for s, e in ts_list:
        scaled.append([s * 1000.0, e * 1000.0])
    return scaled
# ======================================================

# ======================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='argparse testing')
    parser.add_argument('--lang', '-l', type=str, default="zh", help="language")
    parser.add_argument('--share', '-s', action='store_true', help="if to establish gradio share link")
    parser.add_argument('--port', '-p', type=int, default=7860, help='port number')
    parser.add_argument('--listen', action='store_true', help="if to listen to all hosts")
    args = parser.parse_args()

    # ===== ASR æ¨¡å‹è£…è½½ï¼šä¸­æ–‡/è‹±æ–‡ä¸¤å¥— =====
    if args.lang == 'zh':
        funasr_model = AutoModel(
            model="iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
            vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
            spk_model="damo/speech_campplus_sv_zh-cn_16k-common",
        )
    else:
        funasr_model = AutoModel(
            model="iic/speech_paraformer_asr-en-16k-vocab4199-pytorch",
            vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
            spk_model="damo/speech_campplus_sv_zh-cn_16k-common",
        )

    audio_clipper = VideoClipper(funasr_model)
    audio_clipper.lang = args.lang

    server_name = '127.0.0.1'
    if args.listen:
        server_name = '0.0.0.0'

    # ======================
    # ===== å›è°ƒå‡½æ•°åŒº =====
    # ======================

    def audio_recog(audio_input, sd_switch, hotwords, output_dir):
        return audio_clipper.recog(audio_input, sd_switch, None, hotwords, output_dir=output_dir)

    def video_recog(video_input, sd_switch, hotwords, output_dir):
        return audio_clipper.video_recog(video_input, sd_switch, hotwords, output_dir=output_dir)

    def video_clip(dest_text, video_spk_input, start_ost, end_ost, state, output_dir):
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state, dest_spk=video_spk_input, output_dir=output_dir
        )

    def mix_recog(video_input, audio_input, hotwords, output_dir):
        output_dir = output_dir.strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        audio_state, video_state = None, None
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(video_input, 'No', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(audio_input, 'No', hotwords, output_dir=output_dir)
            return res_text, res_srt, None, audio_state

    def mix_recog_speaker(video_input, audio_input, hotwords, output_dir):
        output_dir = output_dir.strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        audio_state, video_state = None, None
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(video_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(audio_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, None, audio_state

    def mix_clip(dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        output_dir = output_dir.strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, dest_spk=video_spk_input, output_dir=output_dir)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, dest_spk=video_spk_input, output_dir=output_dir)
            return None, (sr, res_audio), message, clip_srt

    def video_clip_addsub(dest_text, video_spk_input, start_ost, end_ost, state, output_dir, font_size, font_color):
        output_dir = output_dir.strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state,
            font_size=font_size, font_color=font_color,
            add_sub=True, dest_spk=video_spk_input, output_dir=output_dir
        )

    def llm_inference(system_content, user_content, srt_text, model, apikey):
        SUPPORT_LLM_PREFIX = ['qwen', 'gpt', 'g4f', 'moonshot', 'deepseek']
        if model.startswith('qwen'):
            return call_qwen_model(apikey, model, user_content+'\n'+srt_text, system_content)
        if model.startswith('gpt') or model.startswith('moonshot') or model.startswith('deepseek'):
            return openai_call(apikey, model, system_content, user_content+'\n'+srt_text)
        elif model.startswith('g4f'):
            model2 = "-".join(model.split('-')[1:])
            return g4f_openai_call(model2, system_content, user_content+'\n'+srt_text)
        else:
            logging.error("LLM name error, only {} are supported as LLM name prefix."
                          .format(SUPPORT_LLM_PREFIX))

    def AI_clip(LLM_res, dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        timestamp_list = extract_timestamps(LLM_res)
        output_dir = output_dir.strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state,
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=False)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state,
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=False)
            return None, (sr, res_audio), message, clip_srt

    def AI_clip_subti(LLM_res, dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        timestamp_list = extract_timestamps(LLM_res)
        output_dir = output_dir.strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state,
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=True)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state,
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=timestamp_list, add_sub=True)
            return None, (sr, res_audio), message, clip_srt

    # ============= æ–°å¢ï¼šè¯­ä¹‰åˆ†æè§¦å‘ & ç”¨é«˜å…‰ä¸€é”®å‰ªè¾‘ =============
    def semantic_analyze_action(srt_text, apikey, model):
        """æŒ‰é’®ï¼šæ‰§è¡Œè¯­ä¹‰æ‘˜è¦/å…³é”®è¯/å¤§çº²/é«˜å…‰è¯†åˆ«ï¼Œè¿”å› JSON å­—ç¬¦ä¸²"""
        return semantic_analysis_run(srt_text, apikey, model)

    def semantic_clip_action(analysis_json_str, video_text_input, video_spk_input,
                             start_ost, end_ost, video_state, audio_state, output_dir):
        """æŒ‰é’®ï¼šæŠŠè¯­ä¹‰åˆ†æä¸­çš„é«˜å…‰ highlights ç›´æ¥è½¬æˆå‰ªè¾‘ï¼ˆä¸çƒ§å½•å­—å¹•ï¼‰"""
        ts_list = highlights_to_timestamps(analysis_json_str)  # [[s, e], ...] ç§’

        # âœ… å…³é”®ä¿®å¤ï¼šä¸ºå…¼å®¹åº•å±‚æ—§å®ç°ï¼Œè¿™é‡ŒæŠŠâ€œç§’â€é¢„å…ˆä¹˜ä»¥ 1000 ä½œä¸ºâ€œæ¯«ç§’â€ä¼ ä¸‹å»
        ts_list_scaled = _compat_scale_seconds_for_legacy(ts_list)

        output_dir = (output_dir or "").strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None

        ranges_pretty = ", ".join([f"[{round(s,2)}-{round(e,2)}]" for s, e in ts_list]) if ts_list else "(æ— )"

        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                video_text_input, start_ost, end_ost, video_state,
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=ts_list_scaled, add_sub=False)
            message = f"{message}\n(æŒ‰è¯­ä¹‰é«˜å…‰å‰ªè¾‘: {ranges_pretty})"
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                video_text_input, start_ost, end_ost, audio_state,
                dest_spk=video_spk_input, output_dir=output_dir, timestamp_list=ts_list_scaled, add_sub=False)
            message = f"{message}\n(æŒ‰è¯­ä¹‰é«˜å…‰å‰ªéŸ³é¢‘: {ranges_pretty})"
            return None, (sr, res_audio), message, clip_srt
        return None, None, "æœªå‘ç°å¯ç”¨çš„è¯†åˆ«çŠ¶æ€ï¼ˆvideo_state/audio_state å‡ä¸ºç©ºï¼‰ã€‚è¯·å…ˆè¿›è¡Œè¯†åˆ«ã€‚", ""
    # ===============================================================

    # ======================
    # ===== Gradio UI  =====
    # ======================
    theme = gr.Theme.load("funclip/utils/theme_2.json")
    with gr.Blocks(theme=theme) as funclip_service:
        gr.Markdown(top_md_1)

        video_state, audio_state = gr.State(), gr.State()

        with gr.Row():
            # ===== å·¦ä¾§ï¼šè¾“å…¥ä¸è¯†åˆ« =====
            with gr.Column():
                with gr.Row():
                    video_input = gr.Video(label="è§†é¢‘è¾“å…¥ | Video Input")
                    audio_input = gr.Audio(label="éŸ³é¢‘è¾“å…¥ | Audio Input")

                with gr.Column():
                    hotwords_input = gr.Textbox(label="ğŸš’ çƒ­è¯ | Hotwords(å¯ä»¥ä¸ºç©ºï¼Œä»…æ”¯æŒä¸­æ–‡)")
                    output_dir = gr.Textbox(label="ğŸ“ æ–‡ä»¶è¾“å‡ºè·¯å¾„ | File Output Dir (å¯ä»¥ä¸ºç©º)", value=" ")
                    with gr.Row():
                        recog_button = gr.Button("ğŸ‘‚ è¯†åˆ« | ASR", variant="primary")
                        recog_button2 = gr.Button("ğŸ‘‚ğŸ‘« è¯†åˆ«+åŒºåˆ†è¯´è¯äºº | ASR+SD")

                # å±•ç¤ºç©ºé—´åŠ å¤§ï¼ˆlines=16ï¼‰
                video_text_output = gr.Textbox(label="âœï¸ è¯†åˆ«ç»“æœ | Recognition Result", lines=16, scale=1)
                video_srt_output = gr.Textbox(label="ğŸ“– SRTå­—å¹•å†…å®¹ | RST Subtitles", lines=16, scale=1)

            # ===== å³ä¾§ï¼šLLM æ™ºèƒ½å‰ª / æ–‡æœ¬å‰ª / è¯­ä¹‰åˆ†æ =====
            with gr.Column():
                with gr.Tab("ğŸ§  LLMæ™ºèƒ½è£å‰ª | LLM Clipping"):
                    with gr.Column():
                        prompt_head = gr.Textbox(
                            label="Prompt System",
                            value=("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å­—å¹•åˆ†æä¸å‰ªè¾‘åŠ©æ‰‹ã€‚è¾“å…¥å†…å®¹æ˜¯è§†é¢‘çš„å®Œæ•´ SRT å­—å¹•æ–‡æœ¬ï¼Œè¯·ä½ å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n"
                                   "1. ä»ä¸­é€‰å–æœ€æœ‰ä¿¡æ¯é‡ã€è¯­ä¹‰å®Œæ•´ä¸”è¿ç»­çš„ç‰‡æ®µï¼›\n"
                                   "2. å¯¹æ—¶é—´ä¸Šè¿ç»­çš„å¤šä¸ªå¥å­è¿›è¡Œåˆå¹¶ï¼Œç¡®ä¿æ–‡å­—ä¸æ—¶é—´æˆ³ä¸€ä¸€å¯¹åº”ï¼›\n"
                                   "3. ä¼˜å…ˆé€‰æ‹©è¡¨è¾¾æ ¸å¿ƒè§‚ç‚¹ã€æƒ…æ„Ÿé«˜æ½®æˆ–ä¸»é¢˜è½¬æŠ˜çš„éƒ¨åˆ†ï¼›\n"
                                   "4. æ ¹æ®è§†é¢‘çš„æ€»æ—¶é•¿æˆ–ä¿¡æ¯å¯†åº¦åŠ¨æ€è°ƒæ•´ç‰‡æ®µæ•°é‡ã€‚"
                                   "5. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œæ¯æ¡ç‹¬å ä¸€è¡Œï¼š\n"
                                   "    1. [å¼€å§‹æ—¶é—´-ç»“æŸæ—¶é—´] å†…å®¹æ–‡æœ¬\n"
                                   "    2. [å¼€å§‹æ—¶é—´-ç»“æŸæ—¶é—´] å†…å®¹æ–‡æœ¬\n"
                                   "âš ï¸ ä»…è¾“å‡ºç»“æœï¼›ä½¿ç”¨åŠè§’â€œ-â€ï¼›\n" 
                                   "ä»¥ä¸‹æ˜¯è§†é¢‘çš„è¯­ä¹‰ç†è§£ä¸æ‘˜è¦ä¾›ä½ å‚è€ƒï¼š")
                        )
                        prompt_head2 = gr.Textbox(label="Prompt User", value=("è¿™æ˜¯å¾…è£å‰ªçš„è§†é¢‘srtå­—å¹•ï¼š"))
                        with gr.Column():
                            with gr.Row():
                                llm_model = gr.Dropdown(
                                    choices=[
                                        "deepseek-chat",
                                        "qwen-plus",
                                        "gpt-3.5-turbo",
                                        "gpt-3.5-turbo-0125",
                                        "gpt-4-turbo",
                                        "g4f-gpt-3.5-turbo"
                                    ],
                                    value="deepseek-chat",
                                    label="LLM Model Name",
                                    allow_custom_value=True
                                )
                                apikey_input = gr.Textbox(label="APIKEY")
                            llm_button = gr.Button("LLMæ¨ç† | LLM Inference", variant="primary")
                        llm_result = gr.Textbox(label="LLM Clipper Result", lines=14, scale=1)
                        with gr.Row():
                            llm_clip_button = gr.Button("ğŸ§  LLMæ™ºèƒ½è£å‰ª | AI Clip", variant="primary")
                            llm_clip_subti_button = gr.Button("ğŸ§  LLMæ™ºèƒ½è£å‰ª+å­—å¹• | AI Clip+Subtitles")

                with gr.Tab("âœ‚ï¸ æ ¹æ®æ–‡æœ¬/è¯´è¯äººè£å‰ª | Text/Speaker Clipping"):
                    video_text_input = gr.Textbox(label="âœï¸ å¾…è£å‰ªæ–‡æœ¬ | Text to Clip (å¤šæ®µæ–‡æœ¬ä½¿ç”¨'#'è¿æ¥)")
                    video_spk_input = gr.Textbox(label="âœï¸ å¾…è£å‰ªè¯´è¯äºº | Speaker to Clip (å¤šä¸ªè¯´è¯äººä½¿ç”¨'#'è¿æ¥)")
                    with gr.Row():
                        clip_button = gr.Button("âœ‚ï¸ è£å‰ª | Clip", variant="primary")
                        clip_subti_button = gr.Button("âœ‚ï¸ è£å‰ª+å­—å¹• | Clip+Subtitles")
                    with gr.Row():
                        video_start_ost = gr.Slider(minimum=-500, maximum=1000, value=0, step=50,
                                                    label="âª å¼€å§‹ä½ç½®åç§» | Start Offset (ms)")
                        video_end_ost = gr.Slider(minimum=-500, maximum=1000, value=100, step=50,
                                                  label="â© ç»“æŸä½ç½®åç§» | End Offset (ms)")

                # ===== æ–°å¢ Tabï¼šè¯­ä¹‰ç†è§£ / æ‘˜è¦ / é«˜å…‰æå– =====
                with gr.Tab("ğŸ§© è¯­ä¹‰ç†è§£ä¸æ‘˜è¦ | Semantic Analysis"):
                    with gr.Row():
                        sa_model = gr.Dropdown(
                            choices=[
                                "deepseek-chat",
                                "qwen-plus",
                                "gpt-3.5-turbo",
                                "gpt-3.5-turbo-0125",
                                "gpt-4-turbo",
                                "g4f-gpt-3.5-turbo"
                            ],
                            value="qwen-plus",
                            label="LLM Model Name (Semantic)"
                        )
                        sa_apikey = gr.Textbox(label="APIKEY (Semantic)")
                        sa_button = gr.Button("ğŸ” è¯­ä¹‰æ‘˜è¦/å…³é”®è¯/å¤§çº²/é«˜å…‰", variant="primary")
                    sa_result = gr.Textbox(label="è¯­ä¹‰åˆ†æç»“æœï¼ˆJSONï¼‰", lines=18, scale=1)
                    with gr.Row():
                        sa_clip_button = gr.Button("âœ¨ ç”¨é«˜å…‰ä¸€é”®å‰ªè¾‘ï¼ˆä¸åŠ å­—å¹•ï¼‰", variant="primary")

                with gr.Row():
                    font_size = gr.Slider(minimum=10, maximum=100, value=32, step=2,
                                          label="ğŸ”  å­—å¹•å­—ä½“å¤§å° | Subtitle Font Size")
                    font_color = gr.Radio(["black", "white", "green", "red"],
                                          label="ğŸŒˆ å­—å¹•é¢œè‰² | Subtitle Color", value='white')

                video_output = gr.Video(label="è£å‰ªç»“æœ | Video Clipped")
                audio_output = gr.Audio(label="è£å‰ªç»“æœ | Audio Clipped")
                clip_message = gr.Textbox(label="âš ï¸ è£å‰ªä¿¡æ¯ | Clipping Log", lines=10, scale=1)
                srt_clipped = gr.Textbox(label="ğŸ“– è£å‰ªéƒ¨åˆ†SRTå­—å¹•å†…å®¹ | Clipped RST Subtitles", lines=10, scale=1)

        # ===== äº‹ä»¶ç»‘å®š =====
        recog_button.click(
            mix_recog,
            inputs=[video_input, audio_input, hotwords_input, output_dir],
            outputs=[video_text_output, video_srt_output, video_state, audio_state]
        )
        recog_button2.click(
            mix_recog_speaker,
            inputs=[video_input, audio_input, hotwords_input, output_dir],
            outputs=[video_text_output, video_srt_output, video_state, audio_state]
        )
        clip_button.click(
            mix_clip,
            inputs=[video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, audio_state, output_dir],
            outputs=[video_output, audio_output, clip_message, srt_clipped]
        )
        clip_subti_button.click(
            video_clip_addsub,
            inputs=[video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, output_dir, font_size, font_color],
            outputs=[video_output, clip_message, srt_clipped]
        )
        llm_button.click(
            llm_inference,
            inputs=[prompt_head, prompt_head2, video_srt_output, llm_model, apikey_input],
            outputs=[llm_result]
        )
        llm_clip_button.click(
            AI_clip,
            inputs=[llm_result, video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, audio_state, output_dir],
            outputs=[video_output, audio_output, clip_message, srt_clipped]
        )
        llm_clip_subti_button.click(
            AI_clip_subti,
            inputs=[llm_result, video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, audio_state, output_dir],
            outputs=[video_output, audio_output, clip_message, srt_clipped]
        )

        # æ–°å¢ç»‘å®šï¼šè¯­ä¹‰åˆ†æ & ç”¨é«˜å…‰ä¸€é”®å‰ªè¾‘
        sa_button.click(
            semantic_analyze_action,
            inputs=[video_srt_output, sa_apikey, sa_model],
            outputs=[sa_result]
        )
        sa_clip_button.click(
            semantic_clip_action,
            inputs=[sa_result, video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, audio_state, output_dir],
            outputs=[video_output, audio_output, clip_message, srt_clipped]
        )

    # ===== å¯åŠ¨æœåŠ¡ =====
    if args.listen:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name, inbrowser=False)
    else:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name)



