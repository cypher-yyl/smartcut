import os
import re
import json
import logging
import argparse
import gradio as gr
from funasr import AutoModel                 # FunASR ç»Ÿä¸€å…¥å£ï¼šASR/VAD/æ ‡ç‚¹/è¯´è¯äºº
from videoclipper import VideoClipper        # è¯†åˆ«ã€è£å‰ªã€å­—å¹•ã€å¯¼å‡º
from llm.openai_api import openai_call       # OpenAI/Deepseek/Moonshot
from llm.qwen_api import call_qwen_model     # Qwen ç³»åˆ—
from llm.g4f_openai_api import g4f_openai_call  # å…è´¹ä»£ç†ç±»æ¥å£
from utils.trans_utils import extract_timestamps
from introduction import top_md_1, top_md_3, top_md_4

def _srt_time_to_seconds(t: str) -> float:
    """'HH:MM:SS,ms' æˆ– 'MM:SS,ms' â†’ ç§’(float)ï¼›çº¯æ•°å­—åˆ™å°è¯•ç›´æ¥è½¬ floatã€‚"""
    if not isinstance(t, str):
        return float(t)
    s = t.strip().replace(".", ",")
    m = re.match(r"^(\d{2}):(\d{2}):(\d{2}),(\d{1,3})$", s)
    if m:
        hh, mm, ss, ms = m.groups()
        return int(hh)*3600 + int(mm)*60 + int(ss) + int(ms)/1000.0
    m2 = re.match(r"^(\d{2}):(\d{2}),(\d{1,3})$", s)
    if m2:
        mm, ss, ms = m2.groups()
        return int(mm)*60 + int(ss) + int(ms)/1000.0
    return float(s)  # å…œåº•ï¼šçº¯ç§’/çº¯æ¯«ç§’/å¯è§£ææ•°å­—

def _coerce_ts_to_seconds(ts_list):
    """æŠŠ [[s,e], ...]ï¼ˆå¯èƒ½æ˜¯SRTå­—ç¬¦ä¸²/æ¯«ç§’/ç§’ï¼‰ç»Ÿä¸€æˆ ç§’(float)ã€‚è‡ªåŠ¨è¯†åˆ«>600è§†ä¸ºæ¯«ç§’/æˆ–SRTå·²è½¬ç§’ã€‚"""
    out = []
    for pair in ts_list or []:
        if not isinstance(pair, (list, tuple)) or len(pair) != 2:
            continue
        s, e = pair
        # å­—ç¬¦ä¸² â†’ ç§’
        if isinstance(s, str): s = _srt_time_to_seconds(s)
        if isinstance(e, str): e = _srt_time_to_seconds(e)
        # æ˜æ˜¾åƒæ¯«ç§’ï¼ˆç»éªŒé˜ˆï¼š>600 ä¸” < 1e7ï¼‰â†’ æ¢æˆç§’
        if isinstance(s, (int, float)) and isinstance(e, (int, float)):
            if (s > 600 or e > 600) and (s < 1e7 and e < 1e7):
                s, e = s/1000.0, e/1000.0
        try:
            s = float(s); e = float(e)
            if e > s:  # é•¿åº¦è¿‡æ»¤åé¢å†åš
                out.append([round(s, 3), round(e, 3)])
        except Exception:
            continue
    return out

def _filter_and_clip_to_duration(ts_sec, duration_sec: float, min_len=2.0):
    """è£å‰ªè¶Šç•Œå¹¶è¿‡æ»¤è¿‡çŸ­åŒºé—´ï¼ˆå•ä½ï¼šç§’ï¼‰ã€‚"""
    ok = []
    if not duration_sec or duration_sec <= 0:
        duration_sec = 9e9  # æ²¡æ‹¿åˆ°æ—¶é•¿å°±å…ˆæ”¾è¡Œ
    for s, e in ts_sec:
        s2 = max(0.0, min(float(s), duration_sec))
        e2 = max(0.0, min(float(e), duration_sec))
        if e2 > s2 and (e2 - s2) >= min_len:
            ok.append([round(s2, 3), round(e2, 3)])
    return ok

# ============== è¯­ä¹‰ç†è§£ä¸æ‘˜è¦åˆ†æï¼šPrompt ==============
PROMPT_SEMANTIC_FULL = """ä½ æ˜¯ä¸€ä¸ªè§†é¢‘æ‘˜è¦ä¸å‰ªè¾‘ç­–åˆ’åŠ©æ‰‹ã€‚è¾“å…¥ä¸ºè§†é¢‘çš„å®Œæ•´ SRT å­—å¹•ï¼ˆå«æ—¶é—´ä¸æ–‡æœ¬ï¼‰ã€‚è¯·åŸºäºå†…å®¹å®Œæˆï¼š
1) å½’çº³è§†é¢‘ä¸»é¢˜ï¼›
2) æå–è§†é¢‘ä¸­çš„å…³é”®è¯/å…³é”®çŸ­è¯­ï¼ˆå«ä¸“æœ‰åè¯ï¼‰ï¼›
3) è¾“å‡ºç»“æ„åŒ–å¤§çº²ï¼ˆè‹¥èƒ½ä»å†…å®¹æ¨æ–­ç« èŠ‚ï¼Œè¯·ç»™å‡ºæ¯ç« çš„å¤§è‡´èµ·æ­¢æ—¶é—´ï¼‰ï¼›
4) è¯†åˆ«è§†é¢‘ä¸­çš„â€œç²¾å½©ç‰‡æ®µâ€ï¼Œè¦æ±‚ï¼šè¯­ä¹‰å®Œæ•´ã€æ—¶é—´è¿ç»­ã€èƒ½ä»£è¡¨æ ¸å¿ƒè§‚ç‚¹æˆ–æƒ…ç»ªé«˜æ½®ï¼›æ¯æ¡ç»™å‡ºèµ·æ­¢æ—¶é—´(ç§’)ã€0-1 ç½®ä¿¡åº¦åˆ†ã€ç®€è¦ç†ç”±ã€å¼•ç”¨ä¸€å¥åŸæ–‡ã€‚
5) æŠŠä¸“æœ‰åè¯æŒ‰ç±»åˆ«åˆ†åˆ° entities: {person, org, product, tech_term}ã€‚
6) å¦‚æœ‰å¯¹åç»­å‰ªè¾‘æœ‰ç”¨çš„æç¤ºï¼ˆèŠ‚å¥ã€è½¬æŠ˜ã€ç»“è®ºï¼‰ï¼Œå†™åˆ° notesã€‚

ä¸¥æ ¼è¾“å‡ºå•ä¸ª JSONï¼Œä¸è¦è§£é‡Šæˆ–å¤¹æ‚å¤šä½™æ–‡æœ¬ã€‚æ—¶é—´å•ä½ä½¿ç”¨â€œç§’â€ï¼Œå°æ•°ä¿ç•™ä¸¤ä½ã€‚

ä»¥ä¸‹æ˜¯ SRT æ–‡æœ¬ï¼š
"""

def _safe_json_parse(s: str) -> dict:
    """ä» LLM è¾“å‡ºä¸­æå– JSONï¼›å¤±è´¥åˆ™å°è¯•é¦–å°¾èŠ±æ‹¬å·æˆªå–ã€‚"""
    s = (s or "").strip()
    try:
        return json.loads(s)
    except Exception:
        try:
            head, tail = s.find("{"), s.rfind("}")
            if head != -1 and tail != -1 and head < tail:
                return json.loads(s[head:tail+1])
        except Exception:
            pass
        raise ValueError("LLM æœªè¿”å›åˆæ³• JSONã€‚")

def _call_llm_any(apikey: str, model: str, prompt: str, content: str) -> str:
    """ç»Ÿä¸€çš„ LLM è°ƒç”¨å°è£…ï¼šqwen / gpt / moonshot / deepseek / g4f-*"""
    if model.startswith("qwen"):
        return call_qwen_model(apikey, model, user_input=prompt + content, system_input="")
    if model.startswith("gpt") or model.startswith("moonshot") or model.startswith("deepseek"):
        return openai_call(apikey, model, system_content="", user_content=prompt + content)
    if model.startswith("g4f"):
        pure_model = "-".join(model.split("-")[1:])
        return g4f_openai_call(pure_model, "", prompt + content)
    raise ValueError("Unsupported model prefix. Use one of: qwen / gpt / g4f / moonshot / deepseek")

def semantic_analysis_run(srt_text: str, apikey: str, model: str) -> str:
    """
    è¯­ä¹‰ç†è§£ä¸æ‘˜è¦åˆ†æä¸»å…¥å£ï¼š
    - è¾“å…¥ï¼šSRT æ–‡æœ¬ï¼ˆå¸¦æ—¶é—´ä¸å†…å®¹ï¼‰
    - è¾“å‡ºï¼šJSON å­—ç¬¦ä¸²ï¼ˆåŒ…å« topics/keywords/outline/entities/highlights/notesï¼‰
    """
    if not srt_text or not srt_text.strip():
        return json.dumps({"error": "empty srt"}, ensure_ascii=False, indent=2)
    try:
        raw = _call_llm_any(apikey, model, PROMPT_SEMANTIC_FULL, srt_text)
        data = _safe_json_parse(raw)

        # å…œåº•å­—æ®µ
        data.setdefault("topics", [])
        data.setdefault("keywords", [])
        data.setdefault("outline", [])
        data.setdefault("entities", {"person": [], "org": [], "product": [], "tech_term": []})
        data.setdefault("highlights", [])
        data.setdefault("notes", "")

        return json.dumps(data, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.exception(e)
        return json.dumps({
            "topics": [], "keywords": [], "outline": [],
            "entities": {"person": [], "org": [], "product": [], "tech_term": []},
            "highlights": [], "notes": f"fallback: {str(e)}"
        }, ensure_ascii=False, indent=2)

def highlights_to_timestamps(analysis_json_str: str):
    """
    ä»è¯­ä¹‰åˆ†æ JSON ä¸­æŠ½å–é«˜å…‰ç‰‡æ®µä¸º timestamp_list [[start, end], ...]ï¼ˆå•ä½ï¼šç§’ï¼‰
    - è¿‡æ»¤é•¿åº¦ < 2s çš„è¿‡çŸ­ç‰‡æ®µ
    - æœ€å¤šè¿”å› 6 æ®µ
    """
    try:
        data = json.loads(analysis_json_str)
    except Exception:
        return []
    ts = []
    for h in data.get("highlights", []):
        try:
            s = float(h.get("start", 0))
            e = float(h.get("end", 0))
            if e > s and (e - s) >= 2.0:
                ts.append([round(s, 2), round(e, 2)])
        except Exception:
            continue
    return ts[:6]

# ======== æ–°å¢ï¼šä»è¯­ä¹‰ JSON ç»„è£…ä¸Šä¸‹æ–‡ï¼Œç”¨äºæç¤ºè¯å¢å¼º ========
def build_semantic_context(analysis_json_str: str, max_chars: int = 900) -> str:
    """å°†è¯­ä¹‰åˆ†æ JSON å‹ç¼©æˆå¯æ§é•¿åº¦çš„ä¸Šä¸‹æ–‡æç¤ºï¼›å®¹é”™å„ç§å¥‡æ€ªç»“æ„ã€‚"""
    try:
        data = json.loads(analysis_json_str) if isinstance(analysis_json_str, str) and analysis_json_str.strip() else {}
    except Exception:
        return ""

    if not isinstance(data, dict):
        return ""

    def _to_list(val, n=10):
        if isinstance(val, list):
            out = []
            for x in val[:n]:
                if isinstance(x, (str, int, float)):
                    out.append(str(x))
                elif isinstance(x, dict):
                    # ä»å¸¸è§å­—æ®µå…œåº•å–ä¸€ä¸ªæ–‡æœ¬
                    for k in ("text", "name", "title", "phrase", "kw", "keyword", "desc", "reason"):
                        if k in x and x[k]:
                            out.append(str(x[k]))
                            break
                else:
                    out.append(str(x))
            return out
        return []

    topics = _to_list(data.get("topics"), n=5)
    kws    = _to_list(data.get("keywords"), n=30)

    raw_notes = data.get("notes", "")
    if isinstance(raw_notes, str):
        notes = raw_notes.strip()
    elif isinstance(raw_notes, list):
        notes = " | ".join(_to_list(raw_notes, n=8))
    elif isinstance(raw_notes, dict):
        # æŠŠ dict å‹æˆä¸€è¡Œ
        try:
            notes = json.dumps(raw_notes, ensure_ascii=False)
        except Exception:
            notes = str(raw_notes)
    else:
        notes = str(raw_notes) if raw_notes is not None else ""

    ents = data.get("entities", {}) if isinstance(data.get("entities"), dict) else {}
    persons = _to_list(ents.get("person"), n=10)
    orgs    = _to_list(ents.get("org"), n=10)
    prods   = _to_list(ents.get("product"), n=10)
    techs   = _to_list(ents.get("tech_term"), n=10)

    outline_lines = []
    if isinstance(data.get("outline"), list):
        for o in data["outline"][:8]:
            if isinstance(o, dict):
                title = (o.get("title") or o.get("chapter") or o.get("name") or "").strip()
                s = o.get("start") or o.get("begin") or o.get("from")
                e = o.get("end") or o.get("to")
                if title:
                    outline_lines.append(f"- {title} ({s}-{e}s)" if s is not None and e is not None else f"- {title}")
            elif isinstance(o, str) and o.strip():
                outline_lines.append(f"- {o.strip()}")

    ctx = [
        "### Semantic Context (for clipping)",
        f"Topics: {', '.join(topics)}" if topics else "",
        f"Keywords: {', '.join(kws)}" if kws else "",
        f"Notes: {notes}" if notes else "",
        "Entities:",
        f"  - person: {', '.join(persons)}" if persons else "",
        f"  - org: {', '.join(orgs)}" if orgs else "",
        f"  - product: {', '.join(prods)}" if prods else "",
        f"  - tech_term: {', '.join(techs)}" if techs else "",
        "Outline:",
        *outline_lines
    ]
    text = "\n".join([c for c in ctx if c]).strip()
    return (text[:max_chars] + "â€¦") if len(text) > max_chars else text


# ============= æ–°å¢: å…¼å®¹æ—§ç‰ˆ video_clip çš„æ—¶é—´å•ä½ç¼©æ”¾ =============
def _compat_scale_seconds_for_legacy(ts_list):
    """
    ä¼ å…¥: [[start_sec, end_sec], ...]  (å•ä½: ç§’)
    ç›®çš„: å…¼å®¹åº•å±‚ video_clip/clip ä¸­ 'x16 å† /16000' çš„æ—§é€»è¾‘ã€‚
         ä¸ºäº†è®©æœ€ç»ˆæ—¥å¿—ä»ç„¶æ˜¾ç¤ºä¸ºç§’ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå°†ç§’è½¬æˆ"æ¯«ç§’"ä¼ ä¸‹å»ï¼š
         (sec * 1000) * 16 / 16000 = sec
    è¿”å›: [[start_scaled, end_scaled], ...]  (å•ä½: ä¼ ç»™åº•å±‚çš„æ•°å€¼)
    """
    scaled = []
    for s, e in ts_list:
        scaled.append([s * 1000.0, e * 1000.0])
    return scaled
# ======================================================

# ============= æ–°å¢ï¼šæ—¶é—´ç‰‡æ®µåˆå¹¶å»é‡ï¼ˆç§’ï¼‰ =============
def _merge_timestamp_ranges(ts_a, ts_b, max_segments=6, min_len=2.0):
    """
    ts_a/ts_b: [[s, e], ...]ï¼ˆç§’ï¼‰
    - åˆå¹¶ååšæ’åºã€å»é‡ï¼ˆå…è®¸å°é‡å æ—¶åˆå¹¶ï¼‰
    - è¿‡æ»¤çŸ­ç‰‡æ®µ
    """
    all_ts = []
    for pair in (ts_a or []):
        if isinstance(pair, (list, tuple)) and len(pair) == 2:
            all_ts.append([float(pair[0]), float(pair[1])])
    for pair in (ts_b or []):
        if isinstance(pair, (list, tuple)) and len(pair) == 2:
            all_ts.append([float(pair[0]), float(pair[1])])
    # è¿‡æ»¤å¼‚å¸¸
    all_ts = [p for p in all_ts if p[1] > p[0] and (p[1] - p[0]) >= min_len]
    if not all_ts:
        return []
    # æ’åºå¹¶å¯¹å°é‡å /ç›¸é‚»æ®µè¿›è¡Œåˆå¹¶ï¼ˆé˜ˆå€¼ï¼š0.6sï¼‰
    all_ts.sort(key=lambda x: (x[0], x[1]))
    merged = []
    EPS = 0.6
    for s, e in all_ts:
        if not merged or s > merged[-1][1] + EPS:
            merged.append([round(s, 2), round(e, 2)])
        else:
            merged[-1][1] = round(max(merged[-1][1], e), 2)
    return merged[:max_segments]

# ======================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='argparse testing')
    parser.add_argument('--lang', '-l', type=str, default="zh", help="language")
    parser.add_argument('--share', '-s', action='store_true', help="if to establish gradio share link")
    parser.add_argument('--port', '-p', type=int, default=7860, help='port number')
    parser.add_argument('--listen', action='store_true', help="if to listen to all hosts")
    args = parser.parse_args()

    # ===== ASR æ¨¡å‹è£…è½½ï¼šä¸­æ–‡/è‹±æ–‡ä¸¤å¥— =====
    if args.lang == 'zh':
        funasr_model = AutoModel(
            model="iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
            vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
            spk_model="damo/speech_campplus_sv_zh-cn_16k-common",
        )
    else:
        funasr_model = AutoModel(
            model="iic/speech_paraformer_asr-en-16k-vocab4199-pytorch",
            vad_model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            punc_model="damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
            spk_model="damo/speech_campplus_sv_zh-cn_16k-common",
        )

    audio_clipper = VideoClipper(funasr_model)
    audio_clipper.lang = args.lang

    server_name = '127.0.0.1'
    if args.listen:
        server_name = '0.0.0.0'

    # ======================
    # ===== å›è°ƒå‡½æ•°åŒº =====
    # ======================

    def audio_recog(audio_input, sd_switch, hotwords, output_dir):
        return audio_clipper.recog(audio_input, sd_switch, None, hotwords, output_dir=output_dir)

    def video_recog(video_input, sd_switch, hotwords, output_dir):
        return audio_clipper.video_recog(video_input, sd_switch, hotwords, output_dir=output_dir)

    def video_clip(dest_text, video_spk_input, start_ost, end_ost, state, output_dir):
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state, dest_spk=video_spk_input, output_dir=output_dir
        )

    def mix_recog(video_input, audio_input, hotwords, output_dir):
        output_dir = output_dir.strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        audio_state, video_state = None, None
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(video_input, 'No', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(audio_input, 'No', hotwords, output_dir=output_dir)
            return res_text, res_srt, None, audio_state

    def mix_recog_speaker(video_input, audio_input, hotwords, output_dir):
        output_dir = output_dir.strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        audio_state, video_state = None, None
        if video_input is not None:
            res_text, res_srt, video_state = video_recog(video_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, video_state, None
        if audio_input is not None:
            res_text, res_srt, audio_state = audio_recog(audio_input, 'Yes', hotwords, output_dir=output_dir)
            return res_text, res_srt, None, audio_state

    def mix_clip(dest_text, video_spk_input, start_ost, end_ost, video_state, audio_state, output_dir):
        output_dir = output_dir.strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state, dest_spk=video_spk_input, output_dir=output_dir)
            return clip_video_file, None, message, clip_srt
        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state, dest_spk=video_spk_input, output_dir=output_dir)
            return None, (sr, res_audio), message, clip_srt

    def video_clip_addsub(dest_text, video_spk_input, start_ost, end_ost, state, output_dir, font_size, font_color):
        output_dir = output_dir.strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        return audio_clipper.video_clip(
            dest_text, start_ost, end_ost, state,
            font_size=font_size, font_color=font_color,
            add_sub=True, dest_spk=video_spk_input, output_dir=output_dir
        )

    # ======== æ”¹é€ ï¼šæ”¯æŒâ€œè¯­ä¹‰ä¸Šä¸‹æ–‡å¢å¼ºâ€çš„ LLM æ¨ç† ========
    def llm_inference(system_content, user_content, srt_text, model, apikey,
                      use_semantic_context=False, semantic_json_str=""):
        """
        å…¼å®¹æ—§ç”¨æ³•ï¼šåä¸¤ä¸ªå‚æ•°æœ‰é»˜è®¤å€¼ã€‚å‰ç«¯è‹¥ä¸ä¼ ï¼Œé€»è¾‘ä¸åŸç‰ˆä¸€è‡´ã€‚
        å½“ use_semantic_context=True æ—¶ï¼Œä¼šæŠŠ build_semantic_context(semantic_json_str)
        ä»¥â€œè¡¥å……è¯´æ˜â€æ‹¼æ¥åˆ° user_content ä¹‹å‰ï¼Œå¸®åŠ©æ¨¡å‹é€‰æ®µã€‚
        """
        SUPPORT_LLM_PREFIX = ['qwen', 'gpt', 'g4f', 'moonshot', 'deepseek']
        # æ„é€ ä¸Šä¸‹æ–‡
        extra_ctx = ""
        if use_semantic_context and semantic_json_str:
            extra_ctx = build_semantic_context(semantic_json_str)
            if extra_ctx:
                # å°†ä¸Šä¸‹æ–‡æ”¾åˆ°ç”¨æˆ·æç¤ºå‰ï¼ˆé¿å…ç ´å system çš„è§’è‰²ï¼‰
                user_content = f"[Semantic Context]\n{extra_ctx}\n\n[Task]\n{user_content}"

        if model.startswith('qwen'):
            return call_qwen_model(apikey, model, user_content+'\n'+srt_text, system_content)
        if model.startswith('gpt') or model.startswith('moonshot') or model.startswith('deepseek'):
            return openai_call(apikey, model, system_content, user_content+'\n'+srt_text)
        elif model.startswith('g4f'):
            model2 = "-".join(model.split('-')[1:])
            return g4f_openai_call(model2, system_content, user_content+'\n'+srt_text)
        else:
            logging.error("LLM name error, only {} are supported as LLM name prefix."
                          .format(SUPPORT_LLM_PREFIX))

    # ======== æ”¹é€ ï¼šAI å‰ªè¾‘æ”¯æŒè¯­ä¹‰é«˜å…‰å›é€€ / åˆå¹¶ ========
    def AI_clip(LLM_res, dest_text, video_spk_input, start_ost, end_ost,
                video_state, audio_state, output_dir,
                use_semantic_context=False, semantic_json_str=""):

        # 1) LLM æå– + è¯­ä¹‰é«˜å…‰ â†’ ç»Ÿä¸€åˆ°â€œç§’â€
        raw_llm = extract_timestamps(LLM_res) or []
        ts_from_llm = _coerce_ts_to_seconds(raw_llm)

        ts_from_sa = []
        if use_semantic_context and semantic_json_str:
            ts_from_sa = _coerce_ts_to_seconds(highlights_to_timestamps(semantic_json_str))

        ts_merged = _merge_timestamp_ranges(ts_from_llm, ts_from_sa, max_segments=6, min_len=2.0)

        # 2) è·å–ç´ ææ—¶é•¿ï¼ˆå¯é€‰ï¼šä» state ä¸Šå–ï¼›æ²¡æœ‰å°±ä¸æˆªæ–­ï¼‰
        duration_sec = None
        try:
            media_state = video_state if video_state is not None else audio_state
            duration_sec = float(getattr(media_state, "duration", None)) if media_state is not None else None
        except Exception:
            duration_sec = None

        ts_final = _filter_and_clip_to_duration(ts_merged, duration_sec or 0.0, min_len=2.0)

        output_dir = (output_dir or "").strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        ranges_pretty = ", ".join([f"[{s}-{e}]" for s, e in ts_final]) if ts_final else "(æ— )"

        # 3) ç›´æ¥æŠŠâ€œç§’â€ä¼ ç»™åº•å±‚ï¼ˆmoviepy å°±æ˜¯ç§’ï¼‰
        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state,
                dest_spk=video_spk_input, output_dir=output_dir,
                timestamp_list=ts_final, add_sub=False)
            suffix = "\n(å¯ç”¨è¯­ä¹‰ä¸Šä¸‹æ–‡)" if (use_semantic_context and semantic_json_str) else ""
            message = f"{message}\n(æŒ‰ LLM/è¯­ä¹‰é«˜å…‰åˆå¹¶å‰ªè¾‘: {ranges_pretty}){suffix}"
            return clip_video_file, None, message, clip_srt

        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state,
                dest_spk=video_spk_input, output_dir=output_dir,
                timestamp_list=ts_final, add_sub=False)
            suffix = "\n(å¯ç”¨è¯­ä¹‰ä¸Šä¸‹æ–‡)" if (use_semantic_context and semantic_json_str) else ""
            message = f"{message}\n(æŒ‰ LLM/è¯­ä¹‰é«˜å…‰åˆå¹¶å‰ªéŸ³é¢‘: {ranges_pretty}){suffix}"
            return None, (sr, res_audio), message, clip_srt


    
    def AI_clip_subti(LLM_res, dest_text, video_spk_input, start_ost, end_ost,
                    video_state, audio_state, output_dir,
                    use_semantic_context=False, semantic_json_str=""):

        raw_llm = extract_timestamps(LLM_res) or []
        ts_from_llm = _coerce_ts_to_seconds(raw_llm)

        ts_from_sa = []
        if use_semantic_context and semantic_json_str:
            ts_from_sa = _coerce_ts_to_seconds(highlights_to_timestamps(semantic_json_str))

        ts_merged = _merge_timestamp_ranges(ts_from_llm, ts_from_sa, max_segments=6, min_len=2.0)

        duration_sec = None
        try:
            media_state = video_state if video_state is not None else audio_state
            duration_sec = float(getattr(media_state, "duration", None)) if media_state is not None else None
        except Exception:
            duration_sec = None

        ts_final = _filter_and_clip_to_duration(ts_merged, duration_sec or 0.0, min_len=2.0)

        output_dir = (output_dir or "").strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        ranges_pretty = ", ".join([f"[{s}-{e}]" for s, e in ts_final]) if ts_final else "(æ— )"

        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                dest_text, start_ost, end_ost, video_state,
                dest_spk=video_spk_input, output_dir=output_dir,
                timestamp_list=ts_final, add_sub=True)
            suffix = "\n(å¯ç”¨è¯­ä¹‰ä¸Šä¸‹æ–‡)" if (use_semantic_context and semantic_json_str) else ""
            message = f"{message}\n(æŒ‰ LLM/è¯­ä¹‰é«˜å…‰åˆå¹¶å‰ªè¾‘+å­—å¹•: {ranges_pretty}){suffix}"
            return clip_video_file, None, message, clip_srt

        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                dest_text, start_ost, end_ost, audio_state,
                dest_spk=video_spk_input, output_dir=output_dir,
                timestamp_list=ts_final, add_sub=True)
            suffix = "\n(å¯ç”¨è¯­ä¹‰ä¸Šä¸‹æ–‡)" if (use_semantic_context and semantic_json_str) else ""
            message = f"{message}\n(æŒ‰ LLM/è¯­ä¹‰é«˜å…‰åˆå¹¶å‰ªéŸ³é¢‘+å­—å¹•: {ranges_pretty}){suffix}"
            return None, (sr, res_audio), message, clip_srt


    # ============= æ–°å¢ï¼šè¯­ä¹‰åˆ†æè§¦å‘ & ç”¨é«˜å…‰ä¸€é”®å‰ªè¾‘ =============
    def semantic_analyze_action(srt_text, apikey, model):
        """æŒ‰é’®ï¼šæ‰§è¡Œè¯­ä¹‰æ‘˜è¦/å…³é”®è¯/å¤§çº²/é«˜å…‰è¯†åˆ«ï¼Œè¿”å› JSON å­—ç¬¦ä¸²"""
        return semantic_analysis_run(srt_text, apikey, model)

    def semantic_clip_action(analysis_json_str, video_text_input, video_spk_input,
                            start_ost, end_ost, video_state, audio_state, output_dir):
        ts_list = _coerce_ts_to_seconds(highlights_to_timestamps(analysis_json_str))

        duration_sec = None
        try:
            media_state = video_state if video_state is not None else audio_state
            duration_sec = float(getattr(media_state, "duration", None)) if media_state is not None else None
        except Exception:
            duration_sec = None

        ts_final = _filter_and_clip_to_duration(ts_list, duration_sec or 0.0, min_len=2.0)

        output_dir = (output_dir or "").strip()
        output_dir = os.path.abspath(output_dir) if output_dir else None
        ranges_pretty = ", ".join([f"[{s}-{e}]" for s, e in ts_final]) if ts_final else "(æ— )"

        if video_state is not None:
            clip_video_file, message, clip_srt = audio_clipper.video_clip(
                video_text_input, start_ost, end_ost, video_state,
                dest_spk=video_spk_input, output_dir=output_dir,
                timestamp_list=ts_final, add_sub=False)
            message = f"{message}\n(æŒ‰è¯­ä¹‰é«˜å…‰å‰ªè¾‘: {ranges_pretty})"
            return clip_video_file, None, message, clip_srt

        if audio_state is not None:
            (sr, res_audio), message, clip_srt = audio_clipper.clip(
                video_text_input, start_ost, end_ost, audio_state,
                dest_spk=video_spk_input, output_dir=output_dir,
                timestamp_list=ts_final, add_sub=False)
            message = f"{message}\n(æŒ‰è¯­ä¹‰é«˜å…‰å‰ªéŸ³é¢‘: {ranges_pretty})"
            return None, (sr, res_audio), message, clip_srt

        return None, None, "æœªå‘ç°å¯ç”¨çš„è¯†åˆ«çŠ¶æ€ï¼ˆvideo_state/audio_state å‡ä¸ºç©ºï¼‰ã€‚è¯·å…ˆè¿›è¡Œè¯†åˆ«ã€‚", ""

    # ===============================================================

    # ======================
    # ===== Gradio UI  =====
    # ======================
    theme = gr.Theme.load("funclip/utils/theme_2.json")
    with gr.Blocks(theme=theme) as funclip_service:
        gr.Markdown(top_md_1)

        video_state, audio_state = gr.State(), gr.State()

        with gr.Row():
            # ===== å·¦ä¾§ï¼šè¾“å…¥ä¸è¯†åˆ« =====
            with gr.Column():
                with gr.Row():
                    video_input = gr.Video(label="è§†é¢‘è¾“å…¥ | Video Input")
                    audio_input = gr.Audio(label="éŸ³é¢‘è¾“å…¥ | Audio Input")

                with gr.Column():
                    hotwords_input = gr.Textbox(label="ğŸš’ çƒ­è¯ | Hotwords(å¯ä»¥ä¸ºç©ºï¼Œä»…æ”¯æŒä¸­æ–‡)")
                    output_dir = gr.Textbox(label="ğŸ“ æ–‡ä»¶è¾“å‡ºè·¯å¾„ | File Output Dir (å¯ä»¥ä¸ºç©º)", value=" ")
                    with gr.Row():
                        recog_button = gr.Button("ğŸ‘‚ è¯†åˆ« | ASR", variant="primary")
                        recog_button2 = gr.Button("ğŸ‘‚ğŸ‘« è¯†åˆ«+åŒºåˆ†è¯´è¯äºº | ASR+SD")

                # å±•ç¤ºç©ºé—´åŠ å¤§ï¼ˆlines=16ï¼‰
                video_text_output = gr.Textbox(label="âœï¸ è¯†åˆ«ç»“æœ | Recognition Result", lines=16, scale=1)
                video_srt_output = gr.Textbox(label="ğŸ“– SRTå­—å¹•å†…å®¹ | RST Subtitles", lines=16, scale=1)

            # ===== å³ä¾§ï¼šLLM æ™ºèƒ½å‰ª / æ–‡æœ¬å‰ª / è¯­ä¹‰åˆ†æ =====
            with gr.Column():
                with gr.Tab("ğŸ§  LLMæ™ºèƒ½è£å‰ª | LLM Clipping"):
                    with gr.Column():
                        prompt_head = gr.Textbox(
                            label="Prompt System",
                            value=("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å­—å¹•åˆ†æä¸å‰ªè¾‘åŠ©æ‰‹ã€‚è¾“å…¥å†…å®¹æ˜¯è§†é¢‘çš„å®Œæ•´ SRT å­—å¹•æ–‡æœ¬ï¼Œè¯·ä½ å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n"
                                   "1. ä»ä¸­é€‰å–æœ€æœ‰ä¿¡æ¯é‡ã€è¯­ä¹‰å®Œæ•´ä¸”è¿ç»­çš„ç‰‡æ®µï¼›\n"
                                   "2. å¯¹æ—¶é—´ä¸Šè¿ç»­çš„å¤šä¸ªå¥å­è¿›è¡Œåˆå¹¶ï¼Œç¡®ä¿æ–‡å­—ä¸æ—¶é—´æˆ³ä¸€ä¸€å¯¹åº”ï¼›\n"
                                   "3. ä¼˜å…ˆé€‰æ‹©è¡¨è¾¾æ ¸å¿ƒè§‚ç‚¹ã€æƒ…æ„Ÿé«˜æ½®æˆ–ä¸»é¢˜è½¬æŠ˜çš„éƒ¨åˆ†ï¼›\n"
                                   "4. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œæ¯æ¡ç‹¬å ä¸€è¡Œï¼š\n"
                                   "1. [å¼€å§‹æ—¶é—´-ç»“æŸæ—¶é—´] å†…å®¹æ–‡æœ¬\n"
                                   "2. [å¼€å§‹æ—¶é—´-ç»“æŸæ—¶é—´] å†…å®¹æ–‡æœ¬\n"
                                   "âš ï¸ ä»…è¾“å‡ºç»“æœï¼›ä½¿ç”¨åŠè§’â€œ-â€ï¼›")
                        )
                        prompt_head2 = gr.Textbox(label="Prompt User", value=("è¿™æ˜¯å¾…è£å‰ªçš„è§†é¢‘srtå­—å¹•ï¼š"))
                        with gr.Column():
                            with gr.Row():
                                llm_model = gr.Dropdown(
                                    choices=[
                                        "deepseek-chat",
                                        "qwen-plus",
                                        "gpt-3.5-turbo",
                                        "gpt-3.5-turbo-0125",
                                        "gpt-4-turbo",
                                        "g4f-gpt-3.5-turbo"
                                    ],
                                    value="deepseek-chat",
                                    label="LLM Model Name",
                                    allow_custom_value=True
                                )
                                apikey_input = gr.Textbox(label="APIKEY")
                            # ==== æ–°å¢ï¼šæ˜¯å¦å¯ç”¨è¯­ä¹‰ä¸Šä¸‹æ–‡å¢å¼º ====
                            use_sa_ckb = gr.Checkbox(value=True, label="ğŸ”— ä½¿ç”¨è¯­ä¹‰ä¸Šä¸‹æ–‡å¢å¼ºï¼ˆæ¥è‡ªã€è¯­ä¹‰ç†è§£ä¸æ‘˜è¦ã€Tabï¼‰")
                            llm_button = gr.Button("LLMæ¨ç† | LLM Inference", variant="primary")
                        llm_result = gr.Textbox(label="LLM Clipper Result", lines=14, scale=1)
                        with gr.Row():
                            llm_clip_button = gr.Button("ğŸ§  LLMæ™ºèƒ½è£å‰ª | AI Clip", variant="primary")
                            llm_clip_subti_button = gr.Button("ğŸ§  LLMæ™ºèƒ½è£å‰ª+å­—å¹• | AI Clip+Subtitles")

                with gr.Tab("âœ‚ï¸ æ ¹æ®æ–‡æœ¬/è¯´è¯äººè£å‰ª | Text/Speaker Clipping"):
                    video_text_input = gr.Textbox(label="âœï¸ å¾…è£å‰ªæ–‡æœ¬ | Text to Clip (å¤šæ®µæ–‡æœ¬ä½¿ç”¨'#'è¿æ¥)")
                    video_spk_input = gr.Textbox(label="âœï¸ å¾…è£å‰ªè¯´è¯äºº | Speaker to Clip (å¤šä¸ªè¯´è¯äººä½¿ç”¨'#'è¿æ¥)")
                    with gr.Row():
                        clip_button = gr.Button("âœ‚ï¸ è£å‰ª | Clip", variant="primary")
                        clip_subti_button = gr.Button("âœ‚ï¸ è£å‰ª+å­—å¹• | Clip+Subtitles")
                    with gr.Row():
                        video_start_ost = gr.Slider(minimum=-500, maximum=1000, value=0, step=50,
                                                    label="âª å¼€å§‹ä½ç½®åç§» | Start Offset (ms)")
                        video_end_ost = gr.Slider(minimum=-500, maximum=1000, value=100, step=50,
                                                  label="â© ç»“æŸä½ç½®åç§» | End Offset (ms)")

                # ===== æ–°å¢ Tabï¼šè¯­ä¹‰ç†è§£ / æ‘˜è¦ / é«˜å…‰æå– =====
                with gr.Tab("ğŸ§© è¯­ä¹‰ç†è§£ä¸æ‘˜è¦ | Semantic Analysis"):
                    with gr.Row():
                        sa_model = gr.Dropdown(
                            choices=[
                                "deepseek-chat",
                                "qwen-plus",
                                "gpt-3.5-turbo",
                                "gpt-3.5-turbo-0125",
                                "gpt-4-turbo",
                                "g4f-gpt-3.5-turbo"
                            ],
                            value="qwen-plus",
                            label="LLM Model Name (Semantic)"
                        )
                        sa_apikey = gr.Textbox(label="APIKEY (Semantic)")
                        sa_button = gr.Button("ğŸ” è¯­ä¹‰æ‘˜è¦/å…³é”®è¯/å¤§çº²/é«˜å…‰", variant="primary")
                    sa_result = gr.Textbox(label="è¯­ä¹‰åˆ†æç»“æœï¼ˆJSONï¼‰", lines=18, scale=1)
                    with gr.Row():
                        sa_clip_button = gr.Button("âœ¨ ç”¨é«˜å…‰ä¸€é”®å‰ªè¾‘ï¼ˆä¸åŠ å­—å¹•ï¼‰", variant="primary")

                with gr.Row():
                    font_size = gr.Slider(minimum=10, maximum=100, value=32, step=2,
                                          label="ğŸ”  å­—å¹•å­—ä½“å¤§å° | Subtitle Font Size")
                    font_color = gr.Radio(["black", "white", "green", "red"],
                                          label="ğŸŒˆ å­—å¹•é¢œè‰² | Subtitle Color", value='white')

                video_output = gr.Video(label="è£å‰ªç»“æœ | Video Clipped")
                audio_output = gr.Audio(label="è£å‰ªç»“æœ | Audio Clipped")
                clip_message = gr.Textbox(label="âš ï¸ è£å‰ªä¿¡æ¯ | Clipping Log", lines=10, scale=1)
                srt_clipped = gr.Textbox(label="ğŸ“– è£å‰ªéƒ¨åˆ†SRTå­—å¹•å†…å®¹ | Clipped RST Subtitles", lines=10, scale=1)

        # ===== äº‹ä»¶ç»‘å®š =====
        recog_button.click(
            mix_recog,
            inputs=[video_input, audio_input, hotwords_input, output_dir],
            outputs=[video_text_output, video_srt_output, video_state, audio_state]
        )
        recog_button2.click(
            mix_recog_speaker,
            inputs=[video_input, audio_input, hotwords_input, output_dir],
            outputs=[video_text_output, video_srt_output, video_state, audio_state]
        )
        clip_button.click(
            mix_clip,
            inputs=[video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, audio_state, output_dir],
            outputs=[video_output, audio_output, clip_message, srt_clipped]
        )
        clip_subti_button.click(
            video_clip_addsub,
            inputs=[video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, output_dir, font_size, font_color],
            outputs=[video_output, clip_message, srt_clipped]
        )
        # ==== æ”¹é€ ï¼šæŠŠâ€œæ˜¯å¦ä½¿ç”¨è¯­ä¹‰ä¸Šä¸‹æ–‡ + è¯­ä¹‰ JSONâ€ ä¹Ÿä¼ å…¥ ====
        llm_button.click(
            llm_inference,
            inputs=[prompt_head, prompt_head2, video_srt_output, llm_model, apikey_input, use_sa_ckb, sa_result],
            outputs=[llm_result]
        )
        llm_clip_button.click(
            AI_clip,
            inputs=[llm_result, video_text_input, video_spk_input, video_start_ost, video_end_ost,
                    video_state, audio_state, output_dir, use_sa_ckb, sa_result],
            outputs=[video_output, audio_output, clip_message, srt_clipped]
        )
        llm_clip_subti_button.click(
            AI_clip_subti,
            inputs=[llm_result, video_text_input, video_spk_input, video_start_ost, video_end_ost,
                    video_state, audio_state, output_dir, use_sa_ckb, sa_result],
            outputs=[video_output, audio_output, clip_message, srt_clipped]
        )

        # æ–°å¢ç»‘å®šï¼šè¯­ä¹‰åˆ†æ & ç”¨é«˜å…‰ä¸€é”®å‰ªè¾‘
        sa_button.click(
            semantic_analyze_action,
            inputs=[video_srt_output, sa_apikey, sa_model],
            outputs=[sa_result]
        )
        sa_clip_button.click(
            semantic_clip_action,
            inputs=[sa_result, video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, audio_state, output_dir],
            outputs=[video_output, audio_output, clip_message, srt_clipped]
        )

    # ===== å¯åŠ¨æœåŠ¡ =====
    if args.listen:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name, inbrowser=False)
    else:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name)
